{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f7130d8",
   "metadata": {},
   "source": [
    "# RAG 실습: Amazon Bedrock을 이용한 chat assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c339bb",
   "metadata": {},
   "source": [
    "이 노트북은 Amazon Bedrock에 호스팅된 대형 언어 모델(LLM)을 사용하여 챗봇을 만드는 과정을 안내합니다. 또한, 지식 베이스를 활용해 데이터를 벡터화하고 저장하며, 의미 기반 검색을 통해 데이터를 검색하는 방법도 다룹니다. 벡터 인덱스에는 Amazon OpenSearch Serverless를 사용할 것입니다.\n",
    "\n",
    "우리는 LLM과 지식 베이스(KB)와의 연동을 쉽게 하기 위해서 LangChain을 사용할 것입니다. 이 노트북을 진행하면서 Amazon Bedrock 클라이언트 환경 설정, 보안 권한 구성, 그리고 LangChain에서 프롬프트 템플릿을 사용하는 방법을 배우게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286ceaf2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b>\n",
    "    <ul>\n",
    "        <li>이 노트북은 Amazon SageMaker 노트북 <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html\">인스턴스 또는 Amazon SageMaker 스튜디오 노트북</a> <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks.html\">및 Amazon OpenSearch</a> 서버리스를 지원하는 AWS 지역에서만 실행해야</a> 합니다. <a href=\"https://aws.amazon.com/opensearch-service/features/serverless/\">.</li>\n",
    "        <li>이 노트북을 작성할 당시에는 Amazon Bedrock을 <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html#bedrock-regions\">지원되는 AWS</a> 지역에서만 사용할 수 있었습니다. 다른 AWS 지역에서 이 노트북을 실행하는 경우 Amazon Bedrock 클라이언트의 지역 및/또는 엔드포인트 URL 파라미터를 지원되는 AWS 지역 중 하나로 변경해야 합니다. 이 노트북의 <i>가져오기 구성</i> 섹션에 있는 지침을 따르십시오.</li>\n",
    "        <li>이 노트북은 <i>ml.m5.xlarge의</i> 최소 인스턴스 크기로 실행하는 것이 좋습니다.\n",
    "             <ul>\n",
    "             <li><i>아마존 리눅스 2, 주피터 랩 3을</i> 아마존 세이지메이커 노트북 인스턴스의 플랫폼 식별자로 사용하는 경우</li>\n",
    "             <li>(또는)\n",
    "             <li><i>데이터 사이언스 3.0을</i> 아마존 세이지메이커 스튜디오 노트북의 이미지로 사용합니다.</li>\n",
    "        </li><ul>\n",
    "             <li>이 글을 쓰는 시점에서 이 노트북을 실행하는 데 가장 적합한 최신 버전의 커널은\n",
    "             <ul>\n",
    "             <li><i>Amazon SageMaker Notebook 인스턴스에는 conda_python3이 있습니다.</i></li>\n",
    "             <li><i>Amazon SageMaker Studio Notebook에는 python3이 있습니다.</i></li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a31d997",
   "metadata": {},
   "source": [
    "## 1.\t사전 준비\n",
    "\n",
    "사전 준비를 확인하고 완료하세요.\n",
    "\n",
    "### A. 인터넷 접근 확인 및 구성\n",
    "\n",
    "이 노트북은 필요한 소프트웨어 업데이트를 다운로드하고 데이터셋을 가져오기 위해 인터넷에 대한 아웃바운드 접근이 필요합니다. 기본 설정으로 직접 인터넷 접근을 제공하거나, Amazon VPC를 통해 인터넷 접근을 제공할 수 있습니다.\n",
    "\n",
    "### B. 필요한 소프트웨어 라이브러리 설치\n",
    "\n",
    "이 노트북에는 다음 라이브러리들이 필요합니다:\n",
    "\n",
    "- [SageMaker Python SDK 버전 2.x](https://sagemaker.readthedocs.io/en/stable/v2.html)\n",
    "- [Python 3.10.x](https://www.python.org/downloads/release/python-3100/)\n",
    "- [Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)\n",
    "- [LangChain](https://www.langchain.com/)\n",
    "\n",
    "다음 셀을 실행하여 필요한 라이브러리들을 설치하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a7caba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install boto3==1.34.51\n",
    "!pip install botocore==1.34.142\n",
    "!pip install langchain==0.0.339\n",
    "!pip install unstructured==0.11.0\n",
    "!pip install opensearch-py==2.4.2\n",
    "!pip install PyPDF2==3.0.1\n",
    "!pip install requests-aws4auth==1.2.3\n",
    "\n",
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c402cfa",
   "metadata": {},
   "source": [
    "### C. 로깅 설정\n",
    "\n",
    "**a. 시스템 로그 (선택 사항)**\n",
    "\n",
    "시스템 로그는 노트북이 기본 노트북 인스턴스와 상호작용하면서 생성되는 로그를 의미합니다. 예를 들어, 노트북을 로드하거나 저장할 때 생성되는 로그가 이에 해당합니다.\n",
    "\n",
    "이 로그는 노트북 인스턴스가 시작될 때 자동으로 설정됩니다.\n",
    "\n",
    "이 로그는 이 노트북이 실행 중인 동일한 AWS 리전의 Amazon CloudWatch Logs 콘솔을 통해 액세스할 수 있습니다.\n",
    "\n",
    "- 이 노트북을 Amazon SageMaker Notebook 인스턴스에서 실행 중인 경우, 다음 위치로 이동합니다:\n",
    "  - CloudWatch > Log groups > /aws/sagemaker/NotebookInstances > {notebook-instance-name}/jupyter.log\n",
    "- 이 노트북을 Amazon SageMaker Studio Notebook에서 실행 중인 경우, 다음 위치로 이동합니다:\n",
    "  - CloudWatch > Log groups > /aws/sagemaker/studio > {sagmaker-domain-name}/{user-name}/KernelGateway/{notebook-instance-name}\n",
    "  - CloudWatch > Log groups > /aws/sagemaker/studio > {sagmaker-domain-name}/{user-name}/JupyterServer/default\n",
    "\n",
    "이 노트북이 실행 중인 기본 인스턴스의 이름을 확인하려면, 아래의 코드 셀의 주석을 해제하고 실행하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee57e99b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "notebook_name = ''\n",
    "resource_metadata_path = '/opt/ml/metadata/resource-metadata.json'\n",
    "with open(resource_metadata_path, 'r') as metadata:\n",
    "    notebook_name = (json.load(metadata))['ResourceName']\n",
    "print(\"Notebook instance name: '{}'\".format(notebook_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b51b3b1",
   "metadata": {},
   "source": [
    "**b. 애플리케이션 로그**\n",
    "\n",
    "애플리케이션 로그는 이 노트북에서 다양한 코드 셀을 실행할 때 생성되는 로그를 의미합니다. 이를 설정하려면, 아래의 셀을 실행하여 Python 로깅 서비스를 인스턴스화하세요. 필요한 경우 기본 로그 수준과 형식을 구성할 수 있습니다.\n",
    "\n",
    "기본적으로, 이 노트북은 로그를 해당 셀의 출력 콘솔에만 표시합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e240296e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "# Set the logging level and format\n",
    "log_level = logging.INFO\n",
    "log_format = '%(asctime)s - %(levelname)s - %(message)s'\n",
    "logging.basicConfig(level=log_level, format=log_format)\n",
    "\n",
    "# Save these in the environment variables for use in the helper scripts\n",
    "os.environ['LOG_LEVEL'] = str(log_level)\n",
    "os.environ['LOG_FORMAT'] = log_format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de7586e",
   "metadata": {},
   "source": [
    "### D. import 준비\n",
    "나중에 사용할 수 있도록 모든 라이브러리 및 모듈 import를 준비합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058583db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import langchain\n",
    "import opensearchpy\n",
    "import PyPDF2\n",
    "import requests\n",
    "import sagemaker\n",
    "import sys\n",
    "from botocore.config import Config\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.llms import Bedrock\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# Import the helper functions from the 'scripts' folder\n",
    "sys.path.append(os.path.join(os.getcwd(), \"scripts\"))\n",
    "#logging.info(\"Updated sys.path: {}\".format(sys.path))\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ac0771",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.info(\"Python version : {}\".format(sys.version))\n",
    "logging.info(\"Boto3 version : {}\".format(boto3.__version__))\n",
    "logging.info(\"SageMaker Python SDK version : {}\".format(sagemaker.__version__))\n",
    "logging.info(\"LangChain version : {}\".format(langchain.__version__))\n",
    "logging.info(\"OpenSearch Python Client version : {}\".format(opensearchpy.__version__))\n",
    "logging.info(\"PyPDF2 version : {}\".format(PyPDF2.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd439383",
   "metadata": {},
   "source": [
    "### E. AWS 리전 및 boto3 구성 설정\n",
    "현재 AWS 리전(이 노트북이 실행 중인 곳)과 SageMaker 세션을 가져옵니다. 이는 boto3 API를 사용해 일부 클라이언트를 AWS 서비스로 초기화하는 데 사용됩니다. Bedrock 기본 리전으로는 Oregon(us-west-2) 리전을 추천합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129d0df5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the AWS Region, SageMaker Session and IAM Role references\n",
    "my_session = boto3.session.Session()\n",
    "logging.info(\"SageMaker Session: {}\".format(my_session))\n",
    "my_iam_role = sagemaker.get_execution_role()\n",
    "logging.info(\"Notebook IAM Role: {}\".format(my_iam_role))\n",
    "my_region = my_session.region_name\n",
    "logging.info(\"Current AWS Region: {}\".format(my_region))\n",
    "\n",
    "# Explicity set the AWS Region for Amazon Bedrock clients\n",
    "AMAZON_BEDROCK_DEFAULT_REGION = \"us-west-2\"\n",
    "br_region = os.environ.get('AMAZON_BEDROCK_REGION')\n",
    "if br_region is None:\n",
    "    br_region = AMAZON_BEDROCK_DEFAULT_REGION\n",
    "elif len(br_region) == 0:\n",
    "    br_region = AMAZON_BEDROCK_DEFAULT_REGION\n",
    "logging.info(\"AWS Region for Amazon Bedrock: {}\".format(br_region))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f7cdb2",
   "metadata": {},
   "source": [
    "이 노트북에서 사용되는 모든 boto3 클라이언트에 적용될 시간 초과 및 재시도 구성을 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ea6895",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# boto3 클라이언트의 표준 시간 초과 제한을 1분에서 3분으로 늘립니다. 그리고 재시도 제한을 설정합니다.\n",
    "my_boto3_config = Config(\n",
    "    connect_timeout = (60 * 3),\n",
    "    read_timeout = (60 * 3),\n",
    "    retries = {\n",
    "        'max_attempts': 10,\n",
    "        'mode': 'standard'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3825096-247f-4b68-8676-df4e8eb97fc8",
   "metadata": {},
   "source": [
    "### F. Amazon OpenSearch Serverless collection 확인 및 생성\n",
    "\n",
    "이 노트북은 벡터 검색 유형의 Amazon OpenSearch Serverless (AOSS) 컬렉션을 채팅 도우미가 사용할 벡터 데이터베이스로 사용합니다.\n",
    "\n",
    "다음 셀을 실행하여 AOSS 컬렉션이 존재하지 않는지 확인하고 생성하십시오.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f16e24d-ae12-44c1-9bc9-cc90f46ed33d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# AOSS 컬렉션이 존재하는지, 그리고 이 노트북을 통해 생성되었는지를 식별하기 위한 플래그를 설정합니다.\n",
    "aoss_collection_exists = False\n",
    "aoss_collection_created = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435a9713-4cbd-40c2-84c2-04a3b61b7923",
   "metadata": {},
   "source": [
    "> **Note:** \n",
    "이 노트북을 실행하려면 빈 컬렉션을 사용하는 것이 좋습니다. 세션 중에는 기본적으로 Vector search 유형의 빈 컬렉션이 미리 생성되어 바로 사용할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544b1112-0ef4-474b-a862-c989dfda72d9",
   "metadata": {},
   "source": [
    "다음 코드 셀을 실행하여 사용 가능한 첫 번째 AOSS 컬렉션의 세부 정보를 검색합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa303875-f7aa-4919-ac29-1b3f2ffb0226",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the AOSS client\n",
    "aoss_client = boto3.client(\"opensearchserverless\", config = my_boto3_config)\n",
    "\n",
    "# Check and create a collection if none is found\n",
    "collection_id = ''\n",
    "collections = aoss_client.list_collections()['collectionSummaries']\n",
    "if len(collections) == 0:\n",
    "    aoss_collection_exists = False\n",
    "    logging.info(\"No AOSS collections exist.\")\n",
    "else:\n",
    "    aoss_collection_exists = True\n",
    "    logging.info(\"Found an AOSS collection.\")\n",
    "    first_collection = collections[0]\n",
    "    collection_id = first_collection[\"id\"]\n",
    "    collection_name = first_collection[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a458d778-baef-4ace-9f7d-acac512358d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if len(collection_id) == 0:\n",
    "    aoss_collection_exists = False\n",
    "    logging.info(\"No AOSS collections exist.\")\n",
    "else:\n",
    "    aoss_collection_exists = True\n",
    "    logging.info(\"AOSS 컬렉션은 Collectionid: {}; 컬렉션 이름: {} 을 사용합니다.\"\n",
    "                 .format(collection_id, collection_name))\n",
    "    # Print the AWS console URL to the AOSS collection\n",
    "    collection_aws_console_url = \"https://{}.console.aws.amazon.com/aos/home?region={}#opensearch/collections/{}\"\\\n",
    "    .format(my_region, my_region, collection_name)\n",
    "    logging.info(\"이 컬렉션을 보고 싶다면 {} 를 방문하십시오.\".format(collection_aws_console_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d281f5",
   "metadata": {},
   "source": [
    "### G. Amazon Bedrock에서 모델 액세스 활성화\n",
    "\n",
    "> **Note:**\n",
    "Amazon Bedrock에서 모델을 호출하기 전에 [여기](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html)의 지침에 따라 해당 모델에 대한 액세스를 활성화하십시오.\n",
    "\n",
    "\n",
    "다음 셀을 실행하여 앞서 선택한 AWS 리전에 대한 Amazon Bedrock 모델 액세스 페이지 URL을 인쇄합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c21628",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 아마존 베드락 모델 액세스 페이지 URL을 인쇄합니다.\n",
    "logging.info(\"Amazon Bedrock model access page - https://{}.console.aws.amazon.com/bedrock/home?region={}#/modelaccess\"\n",
    "             .format(br_region, br_region))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aecad58",
   "metadata": {},
   "source": [
    "### H. 보안 권한 확인 및 구성\n",
    "\n",
    "이 노트북은 기본 노트북 인스턴스에 연결된 IAM 역할을 사용합니다.이 역할의 이름을 보려면 다음 셀을 실행하십시오.\n",
    "\n",
    "이 IAM 역할에는 다음과 같은 권한이 있어야 합니다.\n",
    "\n",
    " - Amazon Bedrock에서 대규모 언어 모델 (LLM) 을 호출할 수 있는 전체 액세스 권한.\n",
    " - 이전 단계에서 생성한 Amazon OpenSearch 서버리스 컬렉션에 대한 전체 읽기 및 쓰기 권한.\n",
    " - 아마존 CloudWatch 로그에 쓸 수 있는 액세스 권한.\n",
    "\n",
    "또한 이 노트북 인스턴스와 연결된 IAM 역할에 대한 생성, 읽기 및 쓰기 액세스를 제공하려면 Amazon OpenSearch 서버리스 컬렉션에 데이터 액세스 제어를 설정해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa9962f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IAM 역할 ARN 및 콘솔 URL을 인쇄합니다.\n",
    "logging.info(\"This notebook's IAM role is '{}'\".format(my_iam_role))\n",
    "arn_parts = my_iam_role.split('/')\n",
    "logging.info(\"Details of this IAM role are available at https://{}.console.aws.amazon.com/iamv2/home?region={}#/roles/details/{}?section=permissions\"\n",
    "             .format(my_region, my_region, arn_parts[len(arn_parts) - 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfe7a76",
   "metadata": {},
   "source": [
    "### I. 공통 개체 만들기\n",
    "\n",
    "먼저 다음 셀을 실행하여 Amazon Bedrock에서 사용 가능한 모든 모델을 나열하십시오. 그러면 이 노트북에서 사용할 Amazon Bedrock 내의 LLM 및 임베딩 모델을 선택하는 데 도움이 됩니다. 기본적으로 두 모델 모두 온디맨드 요금 모델을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3db845",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Amazon Bedrock에서 사용 가능한 모든 기반 모델을 나열하십시오.\n",
    "models_info = ''\n",
    "bedrock_client = boto3.client(\"bedrock\", region_name = br_region, endpoint_url = \"https://bedrock.{}.amazonaws.com\"\n",
    "                              .format(br_region), config = my_boto3_config)\n",
    "response = bedrock_client.list_foundation_models()\n",
    "model_summaries = response[\"modelSummaries\"]\n",
    "models_info = models_info + \"\\n\"\n",
    "models_info = models_info + \"-\".ljust(125, \"-\") + \"\\n\"\n",
    "models_info = models_info + \"{:<15} {:<30} {:<20} {:<20} {:<40}\".format(\"Provider Name\", \"Model Name\", \"Input Modalities\",\n",
    "                                                          \"Output Modalities\", \"Model Id\")\n",
    "models_info = models_info + \"-\".ljust(125, \"-\")\n",
    "for model_summary in model_summaries:\n",
    "    models_info = models_info + \"\\n\"\n",
    "    models_info = models_info + \"{:<15} {:<30} {:<20} {:<20} {:<40}\".format(model_summary[\"providerName\"],\n",
    "                                                                            model_summary[\"modelName\"],\n",
    "                                                                            \"|\".join(model_summary[\"inputModalities\"]),\n",
    "                                                                            \"|\".join(model_summary[\"outputModalities\"]),\n",
    "                                                                            model_summary[\"modelId\"])\n",
    "models_info = models_info + \"-\".ljust(125, \"-\") + \"\\n\"\n",
    "logging.info(\"Displaying available models in the '{}' Region:\".format(br_region) + models_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4e0247",
   "metadata": {},
   "source": [
    "위 셀을 실행한 결과로부터\n",
    "\n",
    " 1. 원하는 LLM에 해당하는 모델 ID를 선택하고 다음 셀의 llm_model_id 변수 값으로 설정합니다.\n",
    " 2. (선택 사항) model_kwargs 매개변수에 [LLM별 추론 매개변수](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html)를 지정합니다.\n",
    " 3. 원하는 임베딩 모델에 해당하는 모델 ID를 선택하고 다음 셀의 embeddings_model_id 변수 값으로 설정합니다.\n",
    "\n",
    "이제 다음 셀을 실행하여 이 노트북의 이후 단계에서 사용할 공통 객체를 생성하십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33089a9-a599-4701-9082-460722eeadf2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> 이 노트북은 다음 Amazon Bedrock 모델에서 테스트되었습니다:\n",
    "    <li>LLMs: anthropic.claude-v2, anthropic.claude-instant-v1, cohere.command-text-v14, ai21.j2-ultra-v1</li>\n",
    "    <li>Embedding model(s): amazon.titan-embed-text-v1</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae66bf6-efd1-4f48-b84a-69c78fcf932c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Note:</b> 세션 중에는 다음과 같은 Amazon Bedrock 모델만 사용하는 것이 좋습니다:\n",
    "    <li>LLMs: anthropic.claude-instant-v1</li>\n",
    "    <li>Embedding model(s): amazon.titan-embed-text-v1</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c627f9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### Specify the model-ids along with their inference parameters\n",
    "# Model-id of the LLM to be used in the chat assistant\n",
    "llm_model_id = \"anthropic.claude-instant-v1\"\n",
    "temperature = 0.5\n",
    "max_response_token_length = 300\n",
    "# Model-id of the Embeddings model to be used in the chat assistant\n",
    "embeddings_model_id = \"amazon.titan-embed-text-v1\"\n",
    "\n",
    "##### LLM related objects\n",
    "# Create the Amazon Bedrock runtime client\n",
    "bedrock_rt_client = boto3.client(\"bedrock-runtime\", region_name = br_region, config = my_boto3_config)\n",
    "\n",
    "# Create the LangChain client for the LLM using the Bedrock client created above.\n",
    "llm = Bedrock(\n",
    "    model_id = llm_model_id,\n",
    "    model_kwargs = get_model_specific_inference_params(llm_model_id,\n",
    "                                                       temperature,\n",
    "                                                       max_response_token_length),\n",
    "    client = bedrock_rt_client\n",
    ")\n",
    "\n",
    "##### Embeddings related objects\n",
    "# Use the LangChain BedrockEmbeddings class to create the Embeddings client.\n",
    "br_embeddings = BedrockEmbeddings(client = bedrock_rt_client, model_id = embeddings_model_id, region_name = br_region)\n",
    "\n",
    "##### Amazon OpenSearch Serverless (AOSS) related objects\n",
    "# Create the AOSS Python client from the AOSS boto3 client using the helper function \n",
    "# available through ./scripts/helper_functions.py)\n",
    "aoss_py_client = auth_opensearch(host = \"{}.{}.aoss.amazonaws.com\".format(collection_id, my_region),\n",
    "                            service = 'aoss', region = my_region)\n",
    "# Specify the name of the index in the AOSS collection; this will be created later in the notebook\n",
    "index_name = \"rag-with-gen-ai-index\"\n",
    "# Specify the max workers for loading data in parallel into the index\n",
    "max_workers = 8\n",
    "# To access an Opensearch Collection using LangChain, we can use the OpenSearchVectorSearch class.\n",
    "doc_search = OpenSearchVectorSearch(\n",
    "    opensearch_url = \"{}.{}.aoss.amazonaws.com\".format(collection_id, my_region),\n",
    "    index_name = index_name,\n",
    "    embedding_function = br_embeddings)\n",
    "# Set the doc search client to the AOSS Python client\n",
    "doc_search.client = aoss_py_client\n",
    "\n",
    "##### File related objects\n",
    "# Specify the path to the directory that will contain the RAG data\n",
    "rag_dir = os.path.join(os.getcwd(), \"data/rag\")\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(rag_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229ad476-fa4f-46d3-9c8f-1de18d79e81d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### J. Amazon OpenSearch Serverless collection에서 색인 생성\n",
    "\n",
    "Amazon OpenSearch Serverless(AOSS) 컬렉션에서 인덱스를 생성하려면 먼저 인덱스의 스키마를 정의해야 합니다. AOSS를 사용하면 키워드 매칭을 활용하는 단순 검색 색인을 지정하거나 [k-최근접이웃 (k-NN) 검색](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/knn.html)을 활용하는 벡터 검색 기능을 지정할 수 있습니다. 벡터 검색은 일반적인 키워드 매칭 또는 퍼지 매칭 알고리즘 대신 두 텍스트의 [임베딩](https://en.wikipedia.org/wiki/Word_embedding)을 비교한다는 점에서 표준 검색과 다릅니다. 임베딩은 텍스트와 같은 정보를 수치로 표현한 것으로, 이를 다른 임베딩과 비교할 수 있습니다. 임베딩에 대해 자세히 알아보려면 [이 블로그](https://huggingface.co/blog/getting-started-with-embeddings)를 참조하십시오. 벡터 검색 기능을 사용하면 최종 사용자가 채팅 도우미에게 보내는 질문과 의미상 유사한 문서를 검색할 수 있습니다. 이렇게 하면 사용자의 질문에 답하기 위해 LLM에 제공하는 컨텍스트를 개선할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb9a44c-87e3-4b39-85cb-43aeb56eb10e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# k-NN 유형 벡터를 임베딩으로 사용하여 인덱스에 대한 스키마를 정의합니다.\n",
    "knn_index = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": True,\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "           \"content-embedding\": { \n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 1536 # can have dimension up to 10k\n",
    "            },\n",
    "            \"content\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"title\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"source\": {\n",
    "                \"type\": \"text\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Delete the index\n",
    "#aoss_py_client.indices.delete(index = index_name)\n",
    "\n",
    "# Create the index if it does not exist\n",
    "if aoss_py_client.indices.exists(index = index_name):\n",
    "    logging.info(\"AOSS index '{}' already exists.\".format(index_name))\n",
    "else:\n",
    "    logging.info(\"Creating AOSS index '{}'...\".format(index_name))\n",
    "    logging.info(aoss_py_client.indices.create(index = index_name, body = knn_index, ignore = 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72851b77-b08b-4322-940a-5f9977508cd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# AWS 콘솔 URL을 AOSS 인덱스에 출력합니다.\n",
    "index_aws_console_url = collection_aws_console_url + \"/\" + index_name\n",
    "logging.info(\"이 색인을 보려면 {}를 방문하십시오.\".format(index_aws_console_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc89ba16-7895-4111-9e8b-2a9eb7b73d2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. 채팅 어시스턴트 빌드하기\n",
    "\n",
    "대규모 언어 모델 (LLM)은 [환각](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)) 경향이 있습니다. LLM 컨텍스트에서의 환각은 확실하지만 사실적으로는 틀린 응답을 제공하는 우리의 모델입니다. 이 응답은 모델이 실제로 정답인지 여부에 관계없이 우리가 듣고 싶어한다고 생각하는 내용을 알려주는 경우가 많습니다. LLM이 잘못된 정보를 제공하지 못하도록 방지하는 한 가지 방법은 [검색 증강 생성 (RAG)](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html) 메커니즘을 사용하는 것입니다.\n",
    "\n",
    "RAG를 사용하면 모델이 학습 데이터로부터 사실을 기억하려고 애쓰는 대신 사실에 근거하여 응답하는 데 사용할 수 있는 올바른 컨텍스트 정보를 모델에 제공할 수 있습니다. RAG를 설정하려면 모델에 관련 소스 문서를 제공하는 데 활용할 수 있는 문서 데이터베이스가 있어야 합니다. 문서 데이터베이스를 설정하는 방법은 여러 가지가 있습니다. 이 노트북에서는 '벡터 검색' 유형의 [아마존 오픈서치 서버리스](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-overview.html) 컬렉션을 사용할 것입니다.\n",
    "\n",
    "[LangChain](https://www.langchain.com/)을 사용하여 채팅 어시스턴트가 수행하는 이벤트의 순서를 조정해 보겠습니다. LangChain은 LLM 애플리케이션 생성을 단순화하기 위해 설계된 프레임워크입니다. 유연한 추상화와 광범위한 툴킷을 제공합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb166eaa-be80-49ea-8514-89db10ad6a37",
   "metadata": {},
   "source": [
    "###  A. Architecture <a id='Architecture'></a>\n",
    "\n",
    "![Architecture](./images/architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cc75e7-73cc-4f13-a581-3c93694438c2",
   "metadata": {},
   "source": [
    "### B. 0a단계: 벡터 데이터베이스에 데이터를 로드할 준비를 합니다.<a id='Step0a'></a>\n",
    "\n",
    "Amazon OpenSearch Serverless (AOSS) 컬렉션은 특정 워크로드 또는 사용 사례를 지원하기 위해 함께 작동하는 하나 이상의 인덱스를 논리적으로 그룹화한 것입니다.\n",
    "\n",
    "이 노트북은 벡터 인덱스를 사용하여 AOSS 컬렉션에 있는 문서를 인덱싱합니다. 처리할 수 있는 데이터의 다양성을 보여주기 위해 일부 HTML 및 PDF 파일에서 데이터를 수집해 보겠습니다. 수집 프로세스 중에 이러한 파일에서 텍스트를 추출하여 더 작은 청크로 분할합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3b8ea4-fdc2-4d7b-990c-d4097f7fb203",
   "metadata": {},
   "source": [
    "#### a. 텍스트 스플리터 초기화<a id='Initialize%20the%20text%20splitter'></a>\n",
    "\n",
    "정보 검색을 위해 문서를 인덱싱할 때 전체 문서를 LLM에 컨텍스트로 제공하는 것은 LLM에 부담을 줄 수 있습니다. 특히 매우 긴 문서의 경우 더욱 그렇습니다. 가장 좋은 방법은 문서를 사용하기 쉬운 부분만 겹치는 청크로 나누는 것입니다. 찾고 있는 답변이 문서의 특정 구절 내에 포함되어 있고 전체 문서를 제공할 필요가 없는 경우가 많기 때문에 이러한 방식으로 문서를 나누면 검색 결과 관련성이 향상되는 경향이 있습니다. \n",
    "\n",
    "랭체인의 [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter)를 사용하여 벡터 데이터베이스에 로드하기 전에 내용을 분할하는 데 사용할 텍스트 분할 객체를 만들어 보겠습니다. 여기서는 각 청크의 크기와 연속된 두 청크 사이의 겹치는 문자 수를 설정하는 간단한 '고정 크기 청킹' 전략을 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6c222e-b976-4c06-b6f2-02b2be43baf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 8000,\n",
    "    chunk_overlap  = 100,\n",
    "    length_function = len,\n",
    "    add_start_index = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185df21f-2b37-41e4-9600-4a7f97752007",
   "metadata": {},
   "source": [
    "#### b. 로드할 HTML 파일을 준비합니다.<a id='Prepare%20HTML%20files%20for%20loading'></a>\n",
    "\n",
    "채팅 어시스턴트가 Amazon Bedrock에 관한 질문에 사실적으로 정확한 정보로 답변할 수 있도록 다음 셀을 실행하여 Amazon Bedrock에 관한 일부 문서를 채우십시오.\n",
    "\n",
    "이 설명서는 HTML 파일로 제공됩니다. 첫 단계로 이러한 파일을 `/data/rag/amazon-bedrock-docs/`라는 로컬 디렉터리에 다운로드합니다. 존재하지 않는 경우 필요한 디렉토리 구조가 생성됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dd4dbd-bf64-433e-b8b7-362b7dcc18c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A simple list of Agents for Amazon Bedrock documentation to index\n",
    "html_link_list = [\n",
    "    \"https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-service.html\",\n",
    "    \"https://docs.aws.amazon.com/bedrock/latest/userguide/setting-up.html\",\n",
    "    \"https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html\",\n",
    "    \"https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html\",\n",
    "    \"https://docs.aws.amazon.com/bedrock/latest/userguide/embeddings.html\",\n",
    "    \"https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-prepare.html\",\n",
    "    \"https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-guidelines.html\",\n",
    "    \"https://docs.aws.amazon.com/bedrock/latest/userguide/prov-throughput.html\",\n",
    "    \"https://docs.aws.amazon.com/bedrock/latest/userguide/agents.html\",\n",
    "    \"https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html\"\n",
    "]\n",
    "\n",
    "# Set the sub-directory to store these HTML files\n",
    "html_files_dir = \"{}/{}\".format(rag_dir, \"amazon-bedrock-docs\")\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(html_files_dir, exist_ok = True)\n",
    "\n",
    "# Download and store each HTML file; overwrite existing file\n",
    "for html_link in html_link_list:\n",
    "    html_content = requests.get(html_link).content.decode('utf-8')\n",
    "    file_name = html_link.split(\"/\")[-1]\n",
    "    with open('{}/{}'.format(html_files_dir, file_name), \"w\") as f:\n",
    "        f.write(html_content)\n",
    "        logging.info(\"Downloaded file '{}' to '{}'\".format(file_name, html_files_dir))\n",
    "\n",
    "logging.info(\"A total of {} HTML files were downloaded to '{}'.\".format(len(html_link_list), html_files_dir))\n",
    "        \n",
    "# Display the last downloaded HTML file\n",
    "#HTML(html_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7566e3fc-c7d9-4b58-8fc8-598e9557abdb",
   "metadata": {},
   "source": [
    "이제 HTML 문서를 모두 다운로드했으니 HTML 로더를 사용하여 로드해 보겠습니다. 랭체인의 [비정형 HTML 로더](https://python.langchain.com/docs/modules/data_connection/document_loaders/html)를 사용할 것입니다. 그러면 원시 HTML 파일이 파싱되어 LLM에 제공할 수 있는 형식으로 저장됩니다.마지막으로 위에서 정의한 스플리터 구성에 따라 각 문서를 분할합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f0c7d1-970e-4f48-8515-841d605635b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "\n",
    "# NLTK 데이터 디렉토리 설정\n",
    "nltk_data_dir = '/home/ec2-user/nltk_data'\n",
    "if not os.path.exists(nltk_data_dir):\n",
    "    os.makedirs(nltk_data_dir)\n",
    "\n",
    "nltk.data.path.append(nltk_data_dir)\n",
    "\n",
    "# 리소스 다운로드\n",
    "nltk.download('all', download_dir=nltk_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97074bf-84e0-4c51-97da-7b57e966063d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredHTMLLoader\n",
    "\n",
    "# Initialize\n",
    "html_doc_data_list = []\n",
    "html_doc_link_list = []\n",
    "\n",
    "# Loop through the downloaded HTML files in the directory\n",
    "for html_link in html_link_list:\n",
    "    file_name = html_link.split(\"/\")[-1]\n",
    "    file_path = '{}/{}'.format(html_files_dir, file_name)\n",
    "    \n",
    "    # Load the file content\n",
    "    loader = UnstructuredHTMLLoader(file_path)\n",
    "    data = loader.load()\n",
    "    \n",
    "    # Remove irrelevant text\n",
    "    html_doc = data[0].page_content.replace(\"\"\"Did this page help you? - Yes\n",
    "\n",
    "Thanks for letting us know we're doing a good job!\n",
    "\n",
    "If you've got a moment, please tell us what we did right so we can do more of it.\n",
    "\n",
    "Did this page help you? - No\n",
    "\n",
    "Thanks for letting us know this page needs work. We're sorry we let you down.\n",
    "\n",
    "If you've got a moment, please tell us how we can make the documentation better.\"\"\", \"\").replace(\"\"\"Javascript is disabled or is unavailable in your browser.\n",
    "\n",
    "To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\"\"\", \"\")\n",
    "    \n",
    "    # Split our document into chunks\n",
    "    texts = text_splitter.create_documents([html_doc])\n",
    "    \n",
    "    # Create a list of document chunks as well as a list of links\n",
    "    for text in texts:\n",
    "        html_doc_data_list.append(text.page_content)\n",
    "        html_doc_link_list.append(html_link)\n",
    "\n",
    "logging.info(\"Created {} chunks from the {} downloaded HTML files.\".format(len(html_doc_data_list), len(html_link_list)))        \n",
    "\n",
    "# Print the first chunk\n",
    "#logging.info(html_doc_data_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6143445-fa7d-4c32-a95b-56a010d001ef",
   "metadata": {},
   "source": [
    "#### c. 로드할 PDF 파일 준비<a id='Prepare%20PDF%20files%20for%20loading'></a>\n",
    "\n",
    "다음 셀을 실행하여 일부 제너레이티브 AI 관련 백서를 채우면 채팅 어시스턴트가 관련 질문에 사실적으로 정확한 정보로 답변할 수 있습니다.\n",
    "\n",
    "이 백서는 PDF 파일로 제공됩니다. 첫 단계로 이러한 파일을 `/data/rag/gen-ai-whitepapers/`라는 로컬 디렉터리에 다운로드합니다. 존재하지 않는 경우 필요한 디렉토리 구조가 생성됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434d2b35-d6aa-47d9-a44a-58bd7797ee41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A simple list of Gen AI whitepapers to index\n",
    "pdf_link_list = [\n",
    "    \"https://www-cdn.anthropic.com/bd2a28d2535bfb0494cc8e2a3bf135d2e7523226/Model-Card-Claude-2.pdf\"\n",
    "]\n",
    "\n",
    "# Set the sub-directory to store these PDF files\n",
    "pdf_files_dir = \"{}/{}\".format(rag_dir, \"gen-ai-whitepapers\")\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(pdf_files_dir, exist_ok = True)\n",
    "\n",
    "# Download and store each PDF file; overwrite existing file\n",
    "for pdf_link in pdf_link_list:\n",
    "    pdf_content = requests.get(pdf_link).content\n",
    "    file_name = pdf_link.split(\"/\")[-1]\n",
    "    with open('{}/{}'.format(pdf_files_dir, file_name), \"wb\") as f:\n",
    "        f.write(pdf_content)\n",
    "        logging.info(\"Downloaded file '{}' to '{}'\".format(file_name, pdf_files_dir))\n",
    "\n",
    "logging.info(\"A total of {} PDF files were downloaded to '{}'.\".format(len(pdf_link_list), pdf_files_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1a1e52-8891-40ff-b8fc-8b7b5b837072",
   "metadata": {},
   "source": [
    "이제 PDF 문서를 모두 다운로드했습니다. 이제 처리해 보겠습니다. [PyPDF2](https://pypdf2.readthedocs.io/en/3.0.0/) 라이브러리를 사용하여 텍스트를 추출해 보겠습니다. 마지막으로 위에서 정의한 스플리터 구성에 따라 각 문서를 분할합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f81a88-0059-4c18-9340-2444e0d28cc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Initialize\n",
    "pdf_doc_data_list = []\n",
    "pdf_doc_link_list = []\n",
    "\n",
    "# Loop through the downloaded PDF files in the directory\n",
    "for pdf_link in pdf_link_list:\n",
    "    pdf_doc = ''\n",
    "    file_name = pdf_link.split(\"/\")[-1]\n",
    "    file_path = '{}/{}'.format(pdf_files_dir, file_name)\n",
    "    \n",
    "    # Load the file content\n",
    "    reader = PdfReader(file_path)\n",
    "    \n",
    "    # Loop through the pages\n",
    "    for page in reader.pages:\n",
    "        pdf_doc = pdf_doc + page.extract_text()\n",
    "    \n",
    "    # Split our document into chunks\n",
    "    texts = text_splitter.create_documents([pdf_doc])\n",
    "    \n",
    "    # Create a list of document chunks as well as a list of links\n",
    "    for text in texts:\n",
    "        pdf_doc_data_list.append(text.page_content)\n",
    "        pdf_doc_link_list.append(pdf_link)\n",
    "\n",
    "logging.info(\"Created {} chunks from the {} downloaded PDF files.\".format(len(pdf_doc_data_list), len(pdf_link_list)))        \n",
    "\n",
    "# Print the first chunk\n",
    "#logging.info(pdf_doc_data_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e866b9d-2981-4f95-87f1-9d1977a736f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### C. 0b단계 및 0c단계: 임베딩 생성<a id='Step0band0c'></a>\n",
    "\n",
    "이제 문서 청크가 준비되었으니, 이를 벡터화하여 임베딩을 만들어 보겠습니다. 이와 함께 AOSS 컬렉션의 색인에 삽입할 문서를 준비하겠습니다.\n",
    "\n",
    "준비된 각 문서에는 다음 필드가 포함됩니다.\n",
    "- `content` - 문서 청크의 실제 텍스트를 포함합니다.\n",
    "- `콘텐츠 임베딩` - 해당 문서 청크의 임베딩을 포함합니다.\n",
    "- `제목` - 콘텐츠가 인제스트된 위치의 이름에 대한 참조입니다.\n",
    "- `소스` - 콘텐츠가 인제스트된 위치에 대한 참조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e67cb1-01bd-4eb8-827e-1b48391179d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 문서 청크에서 임베딩을 생성하고 인덱싱할 문서를 준비합니다.\n",
    "# 에서 사용할 수 있는 'prere_index_document_list' 도우미 함수를 사용합니다. /scripts/helper_functions.py\n",
    "\n",
    "# 청크된 HTML 문서 처리하기\n",
    "html_doc_list = prepare_index_document_list(br_embeddings, 'HTML', html_doc_data_list, html_doc_link_list)\n",
    "\n",
    "# 청크된 PDF 문서 처리\n",
    "pdf_doc_list = prepare_index_document_list(br_embeddings, 'PDF', pdf_doc_data_list, pdf_doc_link_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8a9cdd-2060-4b64-b954-6b55388b4340",
   "metadata": {
    "tags": []
   },
   "source": [
    "### D. 0d단계: 벡터 데이터베이스에 임베딩 저장<a id='Step0d'></a>\n",
    "\n",
    "다음 셀을 실행하여 준비된 문서를 생성된 AOSS 컬렉션의 색인에 업로드합니다. 아래 함수는 병렬 처리 함수를 사용하여 문서를 색인에 업로드합니다. 병렬 워커 스레드의 수는 `max_workers` 변수에 의해 제어됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06cd550-4d9e-4e4f-8cc9-dec7e1c9adb0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> 아래 셀을 실행한 후 데이터를 읽을 수 있을 때까지 최대 30초가 걸릴 수 있습니다. boto3 오류가 있는 경우 이전 단계의 설정에 따라 API 호출이 자동으로 재시도됩니다.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac77e7e-1ae0-4a75-933a-cbe1fcd14b63",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "    <b>Note:</b> 이 노트북을 작성할 당시 AOSS는 <i>벡터 검색</i> 컬렉션 유형에 대해 <i>id</i>를 사용하지 않았습니다. 따라서 다음 셀을 두 번 이상 실행하면 AOSS 색인에 중복 문서가 생성됩니다. 이 노트북을 실행하는 용도로는 괜찮습니다.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1568ed25-1c6c-41ad-b519-6237546fcd15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# AOSS 인덱스로 가져올 함수를 정의합니다.\n",
    "def os_import(article):\n",
    "    \"\"\"\n",
    "    This function imports the documents and their metadata into the AOSS index.\n",
    "    \"\"\"\n",
    "    aoss_py_client.index(index = index_name,\n",
    "                         body={\n",
    "                                \"content-embedding\": article['content-embedding'],\n",
    "                                \"content\": article['content'],\n",
    "                                \"title\": article['title'],\n",
    "                                \"source\": article['source'],\n",
    "                              }\n",
    "                        )\n",
    "    \n",
    "# AOSS 컬렉션의 인덱스를 HTML 데이터로 병렬화하고 채웁니다.\n",
    "process_map(os_import, html_doc_list, max_workers = max_workers)\n",
    "\n",
    "# AOSS 컬렉션의 색인을 PDF 데이터로 병렬화하고 채웁니다.\n",
    "process_map(os_import, pdf_doc_list, max_workers = max_workers) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482f165b-5aa3-423a-aea5-badf83e692ab",
   "metadata": {},
   "source": [
    "### E. 1~6단계: 채팅 단계 작성<a id='Step1to6'></a>\n",
    "\n",
    "[검색 증강 생성 (RAG)](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html) 아키텍처의 유용성을 설명하기 위해 벡터 데이터베이스를 검색하지 않고 LLM을 직접 호출해 보겠습니다. 여기서는 Amazon Bedrock 에이전트 또는 Anthropic Claude 모델의 성능에 대해 질문해 보겠습니다. LLM을 교육받았을 당시에는 이러한 기능을 사용할 수 없었기 때문에 LLM이 알지 못할 것으로 알고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5522eb-b277-4a64-b593-7b889c035eb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the query\n",
    "query = \"What are Agents in Amazon Bedrock?\"\n",
    "#query = \"How did Claude 2 do on the Graduate Record Exam?\"\n",
    "query_char_count, query_word_count = get_counts_from_text(query)\n",
    "# Invoke the LLM\n",
    "output = llm.generate([query])\n",
    "# Read the response\n",
    "query_response = output.generations[0][0].text\n",
    "query_resp_char_count, query_resp_word_count = get_counts_from_text(query_response)\n",
    "\n",
    "# Print the details\n",
    "logging.info(\"\\n\\nHuman:\\n{}\\n\\nAssistant:\\n{}\\n\".format(query, query_response))\n",
    "\n",
    "# Print the stats\n",
    "logging.info(\"Query (prompt) stats: Character count = {}; Word count = {}\"\n",
    "             .format(query_char_count, query_word_count))\n",
    "logging.info(\"Response stats: Character count = {}; Word count = {}\"\n",
    "             .format(query_resp_char_count, query_resp_word_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88658cef-02f4-4e3f-8f2a-a77c9fad27cf",
   "metadata": {},
   "source": [
    "이 응답은 그럴듯해 보이지만 실제로는 올바르지 않습니다. 일반적으로 LLM은 질문에 답하려고 노력하지만 이 경우에는 환각 현상이 나타납니다. 이는 LLM이 정답을 제시하지 못한 사례입니다. RAG 아키텍처가 해결책을 제시할 수 있는 곳은 다음과 같습니다.\n",
    "\n",
    "문서 색인에서 Amazon Bedrock 에이전트 또는 Anthropic Claude 모델의 성능에 대한 정보가 포함된 문서를 찾을 수 있는지 확인해 보겠습니다. 다음과 같은 두 가지 방법으로 이 작업을 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3af9433-1fa3-44bd-b83f-cfa8c230cb38",
   "metadata": {},
   "source": [
    "방법 1: AOSS 파이썬 클라이언트의 `검색` 직접 사용하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d3b6eb-c229-468d-b575-0ae3ecf07123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What are Agents in Amazon Bedrock?\"\n",
    "#query = \"How did Claude 2 do on the Graduate Record Exam?\"\n",
    "temp_embedding = br_embeddings.embed_query(text = query)\n",
    "search_query = {\"query\": {\"knn\": {\"content-embedding\": {\"vector\": temp_embedding, \"k\": 5}}}}\n",
    "results = aoss_py_client.search(index = index_name, body = search_query)\n",
    "hits = results[\"hits\"][\"hits\"]\n",
    "logging.info(\"Found {} hit(s).\".format(len(hits)))\n",
    "for hit in hits:\n",
    "    logging.info(hit[\"_source\"][\"source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91611ce7-926d-447e-a69c-27f0b9649b9f",
   "metadata": {},
   "source": [
    "방법 2: 랭체인의 'similarity_search'를 사용하여 AOSS 파이썬 클라이언트를 간접적으로 사용할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405ea102-42bc-4874-9c50-c27dd6cb1c48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_results = 5\n",
    "query = \"What are Agents in Amazon Bedrock?\"\n",
    "#query = \"How did Claude 2 do on the Graduate Record Exam?\"\n",
    "docs = doc_search.similarity_search(\n",
    "    # Our text query\n",
    "    query = query,\n",
    "    # The name of the field that contains our vector\n",
    "    vector_field = \"content-embedding\",\n",
    "    # The actual text field we are looking for\n",
    "    text_field = \"content\",\n",
    "    # The number of results we want to return\n",
    "    k = max_results\n",
    ")\n",
    "logging.info(\"Specified {} max results. Found {} hit(s).\".format(max_results, len(docs)))\n",
    "for doc in docs:\n",
    "    logging.info(docs[0].metadata['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300c46c1-e238-4a59-bd66-e3d41d799690",
   "metadata": {},
   "source": [
    "AOSS 컬렉션의 색인에 준비하고 저장한 문서에 Amazon Bedrock용 에이전트 또는 Falcon LLM용 RefinedWeb 데이터 세트에 대한 정보가 있는 것 같습니다. 이제 문서 색인에 올바른 정보가 있다는 것을 알았으니 [RetrievalQA 체인](https://python.langchain.com/docs/use_cases/question_answering/vector_db_qa) 을 설정해 보겠습니다. 이 체인을 통해 [프롬프트 템플릿](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/), LLM 및 문서 색인을 제공하여 반환된 컨텍스트 문서를 기반으로 질문에 답변하는 질문 응답 체인을 구성할 수 있습니다.[stuff](https://python.langchain.com/docs/modules/chains/document/stuff) 체인 유형을 사용하겠습니다.\n",
    "\n",
    "[프롬프트 템플릿](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/) 은 언어 모델에 대한 프롬프트를 생성하기 위한 사전 정의된 레시피입니다. 아래에서 만든 변수에서는 컨텍스트 및 질문 입력 변수를 지정합니다. 이 변수는 RetrievalQA 체인이 쿼리 및 소스 문서로 채웁니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c395ea-9bea-4ef7-b935-f12ad85ce790",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 프롬프트 템플릿 만들기\n",
    "prompt_template = \"\"\"\\n\\nHuman: \n",
    "다음 컨텍스트를 사용하여 끝에 있는 질문에 답하십시오. \n",
    "답을 모른다면 그냥 모른다고 말하고, 답을 만들려고 하지 마세요. \n",
    "유해한 콘텐츠는 넣지 마세요.\n",
    "모든 답변은 한글로 작성해줘.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {question}\n",
    "\\n\\nAssistant:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template = prompt_template, input_variables = [\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# 검색 QA 체인 생성\n",
    "qa = RetrievalQA.from_chain_type(llm = llm, \n",
    "                                 chain_type = \"stuff\", \n",
    "                                 retriever = doc_search.as_retriever(search_kwargs = {\n",
    "                                     \"vector_field\": \"content-embedding\",\n",
    "                                     \"text_field\": \"content\",\n",
    "                                     \"k\": 5}),\n",
    "                                 return_source_documents = True,\n",
    "                                 chain_type_kwargs = {\"prompt\": PROMPT, \"verbose\": False},\n",
    "                                 verbose = False)\n",
    "\n",
    "# LLM에 질문하고 출처의 참고 자료와 함께 응답을 인쇄하십시오.\n",
    "question = \"What are Agents in Amazon Bedrock?\"\n",
    "#question = \"How did Claude 2 do on the Graduate Record Exam?\"\n",
    "# LangChain RetrievalQA chain을 통해 임베딩 검색, LLM 등을 호출합니다.\n",
    "response = qa(question, return_only_outputs = True)\n",
    "# Parse the output\n",
    "question, answer, context, title, source = parse_rqa_prompt_output(question, response)\n",
    "\n",
    "# Print the details\n",
    "prompt_template_with_result = prompt_template + answer + \"\\n\"\n",
    "logging.info(prompt_template_with_result.format(context=context, question=question))\n",
    "logging.info(\"The context for the above question was retreived from here --> Title: {}; Source: {}\"\n",
    "             .format(title, source))\n",
    "\n",
    "# Print the stats\n",
    "query_char_count, query_word_count = get_counts_from_text(question)\n",
    "logging.info(\"Question stats: Character count = {}; Word count = {}\"\n",
    "             .format(query_char_count, query_word_count))\n",
    "query_resp_char_count, query_resp_word_count = get_counts_from_text(answer)\n",
    "logging.info(\"Answer stats: Character count = {}; Word count = {}\"\n",
    "             .format(query_resp_char_count, query_resp_word_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4581e4-8832-4100-a373-123bf8fabf5b",
   "metadata": {},
   "source": [
    "이제 [대화형 리트리벌 체인](https://python.langchain.com/docs/expression_language/cookbook/retrieval#conversational-retrieval-chain)을 사용하여 한 단계 더 나아가겠습니다. \n",
    "\n",
    "LLM 자체로는 사용자가 마지막으로 입력한 내용이 기억나지 않습니다. 따라서 이전 대화 정보를 기억하고 LLM에 다시 제공할 수 있는 메커니즘이 필요합니다. 이 작업을 수행하려면 [ConversationBufferMemory](https://python.langchain.com/docs/modules/memory/adding_memory) 클래스와 함께 사용할 수 있습니다. 이렇게 하면 LLM과 대화를 진행하고 이전 대화를 메모리에 보관할 수 있습니다.\n",
    "\n",
    "다음 셀은 검색 체인에 대화 요소를 추가하고 검색에 채팅 메모리를 추가할 수 있게 해줍니다. 이 체인은 문서 검색 전에 LLM 호출을 사용하여 대화 기록과 현재 질문을 하나의 새 질문으로 압축하여 문서 검색을 개선합니다.\n",
    "\n",
    "먼저 프롬프트 템플릿을 만들고 체인을 인스턴스화하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae82e945-3d9b-4789-b524-bd564d02592f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 대화형 검색 체인 만들기\n",
    "cqa = ConversationalRetrievalChain.from_llm(llm = llm, \n",
    "                                            chain_type = \"stuff\", \n",
    "                                            condense_question_llm = llm,\n",
    "                                            retriever = doc_search.as_retriever(search_kwargs = {\n",
    "                                                \"vector_field\": \"content-embedding\",\n",
    "                                                \"text_field\": \"content\",\n",
    "                                                \"k\": 5}),\n",
    "                                            return_source_documents = True,\n",
    "                                            memory = ConversationBufferMemory(input_key = \"question\",\n",
    "                                                                              output_key = \"answer\",\n",
    "                                                                              memory_key = \"chat_history\",\n",
    "                                                                              return_messages = True),\n",
    "                                            verbose = False)\n",
    "chat_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35507919-337a-45a0-b868-be42a338fe20",
   "metadata": {},
   "source": [
    "첫 번째 질문을 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d0a703-f0f0-44ff-b9a2-311cfbddf0fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"Claude 2는 대학원 기록 시험에서 어떻게 되었습니까?\"\n",
    "# 랭체인 대화형 검색 체인 체인을 통해 임베딩 검색, LLM 등을 호출합니다.\n",
    "response = cqa.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "# Parse and print the output\n",
    "logging.info(convert_crc_chat_history_to_text(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db0ebce-cb9e-4545-91ff-9c4db16c31cd",
   "metadata": {},
   "source": [
    "두 번째 질문을 합니다. 이제 응답이 인쇄되면 이전 메시지 교환 내용이 메모리의 채팅 기록에 저장되어 그대로 인쇄됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54ab674-15f9-41d5-a000-8b9f0fe16768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"Amazon Bedrock의 agent는 무엇인가요?\"\n",
    "# 랭체인 대화형 검색 체인 체인을 통해 임베딩 검색, LLM 등을 호출합니다.\n",
    "response = cqa.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "# Parse and print the output\n",
    "logging.info(convert_crc_chat_history_to_text(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7370b444",
   "metadata": {},
   "source": [
    "## 3. 어시스턴트와 채팅하기<a id='Chat%20with%20the%20assistant'></a>\n",
    "\n",
    "이제 이 모든 것을 하나의 브라우저 인터페이스에 통합해 보겠습니다. 아래 호출을 통해 노트북 내부의 간단한 대화형 UI가 시작됩니다. 실행해서 질문을 시작하세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18159751",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 'ChatUX' 클래스는 /scripts/helper_functions.py에 정의되어 있습니다.\n",
    "# 인스턴스를 생성하고 어시스턴트와 대화형 채팅을 시작하세요\n",
    "\n",
    "chatux = ChatUX(cqa)\n",
    "chatux.start_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d731f025-0b97-438e-902f-911b3dd3a6b7",
   "metadata": {},
   "source": [
    "## 4. Cleanup <a id='Cleanup'></a>\n",
    "\n",
    "가장 좋은 방법은 더 이상 필요하지 않은 AWS 리소스를 삭제하는 것입니다. 이렇게 하면 불필요한 비용이 발생하지 않도록 할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d229ee71-bb7b-49e7-8e12-b2162e7a8eeb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> 기본적으로 모든 리소스는 세션이 끝날 때 정리됩니다. 세션 외부에서 이 노트북을 실행하는 경우 다음 셀을 실행하여 이 노트북을 통해 생성된 AOSS 리소스를 정리할 수 있습니다.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c82a768-6089-434f-80f7-29e0d18d2fe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 헬퍼 함수 'delete_aoss_collection'(/scripts/helper_functions.py에서 제공하는)는 지정된 항목을 삭제합니다.\n",
    "# AOSS 컬렉션과 그 안의 모든 인덱스.또한 지정된 데이터 액세스 정책, 암호화 정책 및 네트워크 정책도 삭제합니다.\n",
    "if aoss_collection_created:\n",
    "    delete_aoss_collection(aoss_client, collection_id, data_access_policy_name,\n",
    "                           encryption_policy_name, network_policy_name)\n",
    "else:\n",
    "    logging.info(\"Skipping AOSS collection deletion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f13a9a9-0377-43b1-b788-9adc59b9ea56",
   "metadata": {},
   "source": [
    "## 5. Conclusion <a id='Conclusion'></a>\n",
    "\n",
    "이제 Amazon Bedrock에서 호스팅되는 대규모 언어 모델(LLM)을 사용하여 채팅 어시스턴트를 구축하는 방법을 살펴보았습니다. 이 과정에서 RAG(검색 증강 생성) 메커니즘이 환각을 방지하는데 어떻게 도움이 되는지도 시연했습니다. RAG를 사용하면서 Amazon Bedrock에 호스팅된 임베딩 모델을 사용하여 원시 텍스트를 벡터로 변환하는 방법과 Amazon OpenSearch Serverless 컬렉션에서 원시 텍스트를 저장하고 검색하는 방법을 보여 드렸습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0076c69-3b8d-44c1-a7c9-acf0ca9dabbe",
   "metadata": {},
   "source": [
    "## 6. Frequently Asked Questions (FAQs) <a id='FAQs'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66846c0d-0c9d-43ad-9616-27e19d87c583",
   "metadata": {},
   "source": [
    "**Q: 이 노트북에는 어떤 AWS 서비스가 사용됩니까?**\n",
    "\n",
    "노트북을 실행하는 데 사용하는 항목에 따라 Amazon Bedrock, Amazon OpenSearch 서버리스, AWS ID 및 액세스 관리 (IAM), Amazon CloudWatch 및 Amazon SageMaker 노트북 인스턴스 (또는) Amazon SageMaker 스튜디오 노트북 인스턴스가 생성됩니다.\n",
    "\n",
    "**Q: 오픈서치, 아마존 오픈서치 서버리스, 아마존 오픈서치 서비스의 차이점은 무엇입니까?**\n",
    "\n",
    "OpenSearch는 로그 분석, 실시간 애플리케이션 모니터링, 클릭스트림 분석과 같은 사용 사례를 위한 완전한 오픈 소스 검색 및 분석 엔진입니다. 자세한 내용은 [오픈서치 문서](https://opensearch.org/docs/latest/)를 참조하십시오.\n",
    "\n",
    "아마존 오픈서치 서비스는 오픈서치 클러스터를 위한 모든 리소스를 프로비저닝하고 실행합니다. 또한 장애가 발생한 OpenSearch Service 노드를 자동으로 탐지하고 교체하여 자체 관리형 인프라와 관련된 오버헤드를 줄입니다. 단일 API 호출 또는 콘솔에서 몇 번의 클릭으로 클러스터를 확장할 수 있습니다.\n",
    "\n",
    "아마존 오픈서치 서버리스는 아마존 오픈서치 서비스를 위한 온디맨드 서버리스 구성입니다. 서버리스는 OpenSearch 클러스터를 프로비저닝, 구성 및 조정하는 데 따르는 운영상의 복잡성을 제거합니다. OpenSearch 클러스터를 자체 관리하고 싶지 않은 조직이나 대규모 클러스터를 운영할 전담 리소스나 전문 지식이 없는 조직에 적합한 옵션입니다. OpenSearch Serverless를 사용하면 기본 인프라 및 데이터 관리에 대해 걱정할 필요 없이 대량의 데이터를 쉽게 검색하고 분석할 수 있습니다.\n",
    "\n",
    "**Q: Amazon OpenSearch 서버리스는 용량을 어떻게 관리합니까?**\n",
    "\n",
    "Amazon OpenSearch 서버리스를 사용하면 용량을 직접 관리할 필요가 없습니다. OpenSearch 서버리스는 현재 워크로드를 기반으로 계정의 컴퓨팅 파워를 자동으로 확장합니다. 서버리스 컴퓨팅 파워는 오픈서치 컴퓨팅 유닛(OCU) 단위로 측정됩니다. 각 OCU는 6GiB 메모리와 해당 가상 CPU(vCPU), 그리고 Amazon S3로의 데이터 전송의 조합입니다. 오픈서치 서버리스의 분리된 아키텍처에 대한 자세한 내용은 [작동 방식](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-overview.html#serverless-process)을 참조하십시오.\n",
    "\n",
    "**Q: 아마존 베드록이 내 데이터를 캡처하고 저장합니까?**\n",
    "\n",
    "Amazon Bedrock은 AWS 모델을 학습하거나 타사에 배포하는 데 사용자의 프롬프트 및 데이터를 사용하지 않습니다. 학습 데이터는 기본 Amazon Titan 모델을 교육하는 데 사용되거나 제3자에게 배포되지 않습니다. 사용 타임스탬프, 기록된 계정 ID, 서비스에서 기록한 기타 정보 등 기타 사용 데이터도 모델 학습에 사용되지 않습니다.\n",
    "\n",
    "Amazon Bedrock은 Amazon Titan 모델을 미세 조정하는 용도로만 사용자가 제공한 미세 조정 데이터를 사용합니다. Amazon Bedrock은 기본 기반 모델 학습과 같은 다른 용도로는 미세 조정 데이터를 사용하지 않습니다.\n",
    "\n",
    "각 모델 공급자는 모델을 업로드할 수 있는 에스크로 계정을 가지고 있습니다. Amazon Bedrock 추론 계정에는 이러한 모델을 호출할 권한이 있지만, 에스크로 계정 자체에는 Amazon Bedrock 계정에 대한 아웃바운드 권한이 없습니다. 또한 모델 제공자는 Amazon Bedrock 로그에 액세스하거나 고객 프롬프트 및 지속에 액세스할 수 없습니다.\n",
    "\n",
    "Amazon Bedrock은 서비스 로그에 데이터를 저장하거나 기록하지 않습니다.\n",
    "\n",
    "**Q: Amazon Bedrock은 어떤 모델을 지원합니까?**\n",
    "\n",
    "[여기](https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html#models-supported)로 가세요.\n",
    "\n",
    "**Q: Amazon Bedrock의 온디맨드 처리량과 프로비저닝된 처리량의 차이는 무엇입니까?**\n",
    "\n",
    "온디맨드 모드에서는 기간 약정 없이 사용한 만큼만 비용을 지불하면 됩니다. 텍스트 생성 모델의 경우 처리된 모든 입력 토큰과 생성된 모든 출력 토큰에 대해 요금이 부과됩니다. 임베딩 모델의 경우 처리된 모든 입력 토큰에 대해 요금이 부과됩니다. 토큰은 몇 개의 문자로 구성되며, 모델이 사용자 입력을 이해하고 결과를 생성하도록 유도하는 데 학습하는 기본 단위를 말합니다. 이미지 생성 모델의 경우 생성된 모든 이미지에 대해 요금이 부과됩니다.\n",
    "\n",
    "Provisioned Throughput 모드에서는 특정 기본 모델이나 사용자 지정 모델의 모델 단위를 구매할 수 있습니다. 프로비저닝된 처리량 모드는 주로 보장된 처리량이 필요한 대규모의 일관된 추론 워크로드를 위해 설계되었습니다. 사용자 지정 모델은 프로비저닝된 처리량을 사용해야만 액세스할 수 있습니다. 모델 단위는 분당 처리되는 입력 또는 출력 토큰의 최대 수로 측정되는 특정 처리량을 제공합니다. 시간당 요금이 부과되는 이 프로비저닝된 처리량 요금제를 사용하면 1개월 또는 6개월 약정 기간 중에서 유연하게 선택할 수 있습니다.\n",
    "\n",
    "**Q: Amazon Bedrock에 대한 고객 레퍼런스는 어디에서 찾을 수 있습니까?**\n",
    "\n",
    "[여기](https://aws.amazon.com/bedrock/testimonials/)로 가세요.\n",
    "\n",
    "**Q: 신속한 엔지니어링을 위한 리소스는 어디서 찾을 수 있나요?**\n",
    "\n",
    "[프롬프트 엔지니어링 가이드](https://www.promptingguide.ai/).\n",
    "\n",
    "**Q: 아마존 베드록을 사용하려면 랭체인이 필수인가요?**\n",
    "\n",
    "아니요. [베드록 API](https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html) 또는 언어별 [AWS SDK](https://aws.amazon.com/developer/tools/)를 사용하여 아마존 베드록과 상호 작용할 수 있습니다. LangChain을 사용하면 이 아키텍처와 관련된 다양한 구성 요소 간의 상호 작용과 관련된 단계를 간단하게 제어할 수 있습니다. \n",
    "\n",
    "**Q: LangChain을 시작하려면 어떻게 해야 하나요?**\n",
    "\n",
    "[여기](https://python.langchain.com/docs/get_started/introduction)로 가세요.\n",
    "\n",
    "**Q: 이 노트북에서 사용되는 AWS 서비스의 요금 정보는 어디에서 찾을 수 있습니까?**\n",
    "\n",
    "- 아마존 베드락 가격 책정 - [여기](https://aws.amazon.com/bedrock/pricing/)로 이동하세요.\n",
    "<i>- 아마존 오픈서치 서버리스 가격 책정 - [여기](https://aws.amazon.com/opensearch-service/pricing/)로 이동하여 서버리스 섹션으로 이동하십시오.</i>\n",
    "— AWS ID 및 액세스 관리 (IAM) 요금 — 무료.\n",
    "- 아마존 클라우드워치 요금 - [여기](https://aws.amazon.com/cloudwatch/pricing/)로 이동하십시오.\n",
    "- 아마존 세이지메이커 노트북 인스턴스 (또는) 아마존 세이지메이커 스튜디오 노트북 요금 - [여기](https://aws.amazon.com/sagemaker/pricing/)로 이동하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477ba475-2a18-4935-8a67-a95d66522109",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
